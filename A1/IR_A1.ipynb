{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordPunctTokenizer\n",
    "import os, pprint\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import regexp_tokenize\n",
    "from nltk import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_ws = set(stopwords.words('english'))\n",
    "\n",
    "cwd = os.getcwd()\n",
    "dataPath = \"20_newsgroups/\"\n",
    "\n",
    "docs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_documents(dataPath):\n",
    "    \n",
    "    new_path = cwd+'/'+dataPath\n",
    "    every_path = list(os.walk(new_path))\n",
    "    \n",
    "    total = 1\n",
    "    for i in range(0,len(every_path)):\n",
    "        dirPath, dirName, fileNames = every_path[i]\n",
    "        if(len(dirName) == 0):\n",
    "            for j in fileNames:\n",
    "                single_doc_loc = dirPath + '/' + j\n",
    "                os.rename(single_doc_loc, dirPath + '/' + str(total))\n",
    "                total+=1\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename_documents(dataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_documents(dataPath):\n",
    "    global docs\n",
    "    \n",
    "    new_path = cwd+'/'+dataPath\n",
    "    every_path = list(os.walk(new_path))\n",
    "    \n",
    "    for i in range(0,len(every_path)):\n",
    "        dirPath, dirName, fileNames = every_path[i]\n",
    "        if(len(dirName) == 0):\n",
    "            for j in fileNames:\n",
    "                single_doc = []\n",
    "                single_doc_loc = dirPath + '/' + j\n",
    "                with open(single_doc_loc, 'rb') as f:\n",
    "                    single_doc.append(str(f.read()))\n",
    "                docs[j] = single_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_documents(dataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeDocument(document):\n",
    "    \"\"\"\n",
    "        document : string\n",
    "        \n",
    "        0. Convert to lowercase\n",
    "        1. Stop words removed\n",
    "        2. Tokenize \n",
    "        3. Stemming\n",
    "        4. Lemmatization\n",
    "        5. Only words that starts with alphabet or digit. Front 0's removed.\n",
    "    \"\"\"\n",
    "    ts = document.split('\\\\n')\n",
    "    document = ' '.join(ts)\n",
    "    ts = document.split('\\t')\n",
    "\n",
    "    document = ' '.join(ts)\n",
    "  \n",
    "    # Tokenization\n",
    "    tokens = WordPunctTokenizer().tokenize(document)\n",
    "\n",
    "    # lowercase\n",
    "    tokens_lowercase = [ w.lower() for w in tokens]\n",
    "    \n",
    "    #Remove Stop words\n",
    "    tokens_stop  = [ w for w in tokens_lowercase if(w not in stop_ws)] \n",
    "    \n",
    "    # Stemming \n",
    "    tokens_stem = [ PorterStemmer().stem(w) for w in tokens_stop]   # .wes. we\n",
    "\n",
    "    # Lemmatization\n",
    "    updated_tokens = [ WordNetLemmatizer().lemmatize(w) for w in tokens_stem]\n",
    "     \n",
    "    final_tokens = []\n",
    "    \n",
    "    for updated_token in updated_tokens:\n",
    "        if(updated_token[0].isalpha()) and (len(updated_token) > 1):\n",
    "            final_tokens.append(updated_token)\n",
    "        else:\n",
    "            if(updated_token.isnumeric()):\n",
    "                final_tokens.append(str(int(updated_token)))\n",
    "            else:\n",
    "                if(updated_token[0].isdigit()):\n",
    "                    updated_token = updated_token.lstrip('0')\n",
    "                    final_tokens.append(updated_token)\n",
    "        \n",
    "    \n",
    "    final_tokens = final_tokens[1:]  # remove b\n",
    "\n",
    "    # Unique only\n",
    "    final_tokens = list(set(final_tokens))\n",
    "\n",
    "    return final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_docId = []\n",
    "\n",
    "for k,v in docs.items():\n",
    "    for i in tokenizeDocument(v[0]):\n",
    "        token_docId.append([i,int(k)])\n",
    "        \n",
    "token_docId.sort()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Inverted Index ####\n",
    "inverted_index = {}\n",
    "\n",
    "def buildIndex(token_docId):\n",
    "    global inverted_index \n",
    "    \n",
    "    # Inverted Index key,val = (term, list)\n",
    "    inverted_index = {}\n",
    "    for element in token_docId:\n",
    "        term = element[0]\n",
    "        docid = element[1]\n",
    "        if(term not in inverted_index.keys()):\n",
    "            postings_list = [[],[]]\n",
    "            postings_list[0].append(1)\n",
    "            postings_list[1].append(docid)\n",
    "            inverted_index[term] = postings_list\n",
    "        else:\n",
    "            plist = inverted_index[term] \n",
    "            plist[0][0]+=1\n",
    "            plist[1].append(docid)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "buildIndex(token_docId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185376\n"
     ]
    }
   ],
   "source": [
    "print(len(inverted_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the index in a file\n",
    "with open('inverted-index.txt','w') as f:\n",
    "    print(inverted_index, file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeInput(inp):\n",
    "    \"\"\"\n",
    "        inp : Not a stop word\n",
    "        \n",
    "        0. Convert to lowercase\n",
    "        1. Stemming\n",
    "        2. Lemmatization\n",
    "        3. Only words that starts with alphabet or digit.\n",
    "    \"\"\"\n",
    "    \n",
    "    # lowercase\n",
    "    inp = inp.lower()\n",
    "    \n",
    "    # Stemming \n",
    "    inp_stem = PorterStemmer().stem(inp)\n",
    "    \n",
    "    # Lemmatization\n",
    "    inp_lemma = WordNetLemmatizer().lemmatize(inp_stem)\n",
    "    \n",
    "    inp_lemma = inp_lemma.lstrip('0')\n",
    "    \n",
    "    #strip spaces \n",
    "    inp_lemma = inp_lemma.strip()\n",
    "    \n",
    "    return inp_lemma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Query Results  \n",
    "\n",
    "def booleanQuery(query_type, x, y):\n",
    "    \"\"\"\n",
    "        1. x OR y\n",
    "        2. x AND y\n",
    "        3. x AND NOT y\n",
    "        4. X OR NOT y\n",
    "    \"\"\"\n",
    "    \n",
    "    # Preprocess x and y\n",
    "    new_x = tokenizeInput(x)\n",
    "    new_y = tokenizeInput(y)\n",
    "        \n",
    "    # Check key present in keys\n",
    "    x_p = new_x in inverted_index.keys()\n",
    "    y_p = new_y in inverted_index.keys()\n",
    "    \n",
    "    #  print(new_x, new_y, x_p, y_p)\n",
    "    results = 0\n",
    "    ps3 = set()\n",
    "    \n",
    "    if(query_type == 1):\n",
    "        \n",
    "        if(x_p and y_p):               \n",
    "            ps1 = inverted_index[new_x][1]\n",
    "            ps2 = inverted_index[new_y][1]\n",
    "            ps3 = set(ps1) | set(ps2)   # x or y\n",
    "            results = sum(inverted_index[new_x][0] + inverted_index[new_y][0])\n",
    "            \n",
    "        elif(x_p):\n",
    "            ps3 = inverted_index[new_x][1]\n",
    "            results = inverted_index[new_x][0]\n",
    "        else:\n",
    "            ps3 = inverted_index[new_y][1]\n",
    "            results = inverted_index[new_y][0]\n",
    "            \n",
    "    elif(query_type == 2):\n",
    "        \n",
    "        if(x_p and y_p):               \n",
    "            ps1 = inverted_index[new_x][1]\n",
    "            ps2 = inverted_index[new_y][1]\n",
    "            ps3 = set(ps1) & set(ps2)   # x and y\n",
    "            results = len(ps3) \n",
    "        else:\n",
    "            print('No results found.')\n",
    "    \n",
    "    elif(query_type == 3):\n",
    "        # take those from x which are not in y \n",
    "        # x - y == x.y`\n",
    "        if(x_p == False):\n",
    "            print('No results found.')\n",
    "        else:\n",
    "            if(y_p == True):\n",
    "                ps1 = inverted_index[new_x][1]\n",
    "                ps2 = inverted_index[new_y][1]\n",
    "                ps3 = set(ps1) - set(ps2)   # x and y\n",
    "                results = len(ps3)\n",
    "            else:\n",
    "                print('y has no term to map.')\n",
    "    else:\n",
    "        # O(N) x + y` = total - (y.x`) = total -(y - x)\n",
    "        if(y_p == False):\n",
    "            print('No results found.')\n",
    "        else:\n",
    "            if(x_p == True):\n",
    "                ps1 = inverted_index[new_y][1]\n",
    "                ps2 = inverted_index[new_x][1]\n",
    "                ps3 = set(docs.keys()) - (set(ps1) - set(ps2))   # x and y\n",
    "                results = len(ps3)\n",
    "            else:\n",
    "                print('y has no term to map.')\n",
    " \n",
    "    \n",
    "\n",
    "    return ps3, results        \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Select query type ###\n",
      "1. x OR y\n",
      "2. x AND y\n",
      "3. x AND NOT y\n",
      "4. x OR NOT y\n",
      "Select query number2\n",
      "Enter x :scientist\n",
      "Enter y :scientist\n"
     ]
    }
   ],
   "source": [
    "## Take Input Query\n",
    "\n",
    "print(\"## Select query type ###\")\n",
    "print('1. x OR y')\n",
    "print('2. x AND y')\n",
    "\n",
    "print('3. x AND NOT y')\n",
    "print('4. x OR NOT y')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "t = int(input('Select query number'))\n",
    "x = input('Enter x :')\n",
    "y = input('Enter y :')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results found:  267\n",
      "[104, 138, 321, 767, 1194, 1225, 1234, 1284, 1302, 1385, 1449, 1597, 1772, 1904, 1959, 1980, 2681, 2929, 3035, 3062, 3078, 3161, 3188, 3363, 3426, 3458, 3533, 3543, 3610, 3656, 3722, 3899, 3937, 3953, 3967, 4078, 4108, 4176, 4188, 4352, 4369, 4398, 4451, 4490, 4507, 4830, 4833, 4878, 4978, 5002, 5032, 5038, 5062, 5092, 5094, 5166, 5167, 5183, 5202, 5245, 5277, 5297, 5307, 5332, 5362, 5420, 5567, 5569, 5575, 5617, 5640, 5644, 5698, 5727, 5734, 5748, 5860, 5890, 5938, 5995, 5998, 7347, 7408, 7425, 7576, 7609, 7702, 7751, 7753, 7893, 10117, 10321, 10683, 10735, 10794, 10963, 11000, 11391, 11428, 11641, 11739, 11759, 11781, 11817, 11901, 13010, 13020, 13068, 13072, 13085, 13096, 13101, 13133, 13136, 13142, 13156, 13191, 13198, 13201, 13207, 13242, 13252, 13270, 13277, 13289, 13291, 13300, 13305, 13318, 13327, 13333, 13367, 13413, 13434, 13455, 13458, 13463, 13466, 13482, 13494, 13529, 13533, 13547, 13549, 13568, 13572, 13582, 13588, 13608, 13625, 13631, 13645, 13650, 13660, 13666, 13670, 13692, 13706, 13730, 13752, 13761, 13767, 13769, 13783, 13805, 13842, 13850, 13869, 13883, 13890, 13899, 13900, 13901, 13920, 13989, 15004, 15050, 15068, 15069, 15076, 15188, 15255, 15264, 15323, 15330, 15357, 15366, 15380, 15383, 15489, 15620, 15708, 15733, 15762, 15804, 15825, 15853, 15857, 15873, 15876, 15910, 15951, 15966, 16001, 16071, 16127, 16196, 16210, 16310, 16325, 16339, 16355, 16374, 16397, 16401, 16500, 16545, 16567, 16591, 16646, 16663, 16671, 16679, 16709, 16712, 16713, 16715, 16801, 16863, 16873, 16889, 16890, 16897, 18004, 18137, 18240, 18391, 18448, 18489, 18796, 18996, 19084, 19175, 19178, 19192, 19210, 19225, 19231, 19343, 19344, 19379, 19409, 19432, 19435, 19495, 19534, 19567, 19678, 19751, 19773, 19795, 19802, 19883, 19913, 19918, 19945, 19975]\n"
     ]
    }
   ],
   "source": [
    "matching_docs, total_count = booleanQuery(t,x,y)\n",
    "print('Total results found: ', total_count)\n",
    "print(sorted(matching_docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [1257]]\n"
     ]
    }
   ],
   "source": [
    "print(inverted_index['dmorfx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[140], [94, 203, 271, 505, 599, 785, 861, 1257, 1702, 1841, 2039, 2064, 2093, 2127, 2228, 2244, 2288, 2322, 2422, 2531, 2581, 2627, 2686, 2804, 2943, 2977, 3149, 3253, 3320, 3337, 3379, 3421, 3438, 3456, 3491, 3501, 3533, 3604, 3853, 3895, 3928, 3978, 3990, 5237, 5252, 5410, 5453, 5510, 5718, 5908, 5972, 6533, 7045, 7072, 7134, 7170, 7201, 7203, 7239, 7263, 7280, 7315, 7324, 7349, 7360, 7506, 7543, 7578, 7584, 7616, 7634, 7679, 7703, 7721, 7788, 7890, 7904, 7906, 7986, 8075, 8854, 8870, 9153, 9370, 9389, 9504, 9545, 9561, 9640, 10056, 10178, 10335, 10432, 10455, 10456, 10478, 10622, 10653, 10793, 10891, 10898, 12152, 13229, 13414, 13468, 14715, 14869, 14904, 15070, 15094, 15385, 15624, 15688, 15776, 15870, 16846, 17090, 17115, 17397, 17489, 17783, 17865, 17889, 18124, 18321, 18668, 18910, 18940, 19130, 19189, 19203, 19243, 19299, 19320, 19460, 19550, 19602, 19634, 19816, 19918]]\n"
     ]
    }
   ],
   "source": [
    "print(inverted_index['horribl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
