{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import regexp_tokenize\n",
    "from nltk import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordPunctTokenizer\n",
    "import math\n",
    "from num2words import num2words\n",
    "from decimal import Decimal\n",
    "from word2number import w2n\n",
    "\n",
    "stop_ws = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def documentText(file_name):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            file_name (str) : file name \n",
    "        return:\n",
    "            string (str) : file content\n",
    "    \"\"\"\n",
    "\n",
    "    with open(stories_path+'/'+file_name, 'rb') as f:\n",
    "        file_content = str(f.read())\n",
    "    \n",
    "    return file_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(path):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            path (str): index.html file location\n",
    "        Returns:\n",
    "            dataframe:\n",
    "    \"\"\"\n",
    "    \n",
    "    loc = path + 'index.html'\n",
    "    with open(loc, 'r') as f:\n",
    "        data = str(f.read())\n",
    "        \n",
    "    sp = BeautifulSoup(data, \"lxml\")\n",
    "    filename = []\n",
    "    title = []\n",
    "\n",
    "    for t in sp.find_all('tr'):\n",
    "        flag = False\n",
    "\n",
    "        for c in t.find_all('td'):\n",
    "            if(not flag):    \n",
    "                for a in c.find('a'):\n",
    "                    filename.append(a)\n",
    "                    flag = True  \n",
    "            if(c.string != None):\n",
    "                title.append(c.string.rstrip())\n",
    "                flag = False\n",
    "        break\n",
    "\n",
    "    docs_frame = pd.DataFrame(columns = [\"docid\", \"name\", \"title\", \"content\"])\n",
    "    # pprint(list(zip(filename, title)))\n",
    "\n",
    "    f = 1\n",
    "    for i in range(0, len(filename)):\n",
    "        text = documentText(filename[i])\n",
    "        docs_frame = docs_frame.append(pd.Series([f, filename[i], title[i], text], index = [\"docid\", \"name\", \"title\",\"content\"]), ignore_index=True)\n",
    "        f+=1\n",
    "\n",
    "    return docs_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeDocument(docs):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            docs (dict) : all documents\n",
    "        return:\n",
    "            final tokens (list) : terms for index\n",
    "\n",
    "        0. Convert to lowercase\n",
    "        1. Stop words removed\n",
    "        2. Tokenize \n",
    "        3. Stemming\n",
    "        4. Lemmatization\n",
    "        5. Only words that starts with alphabet or digit. Front 0's removed. Num2words\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    ts = docs.split('\\\\n')\n",
    "    docs = ' '.join(ts)\n",
    "    ts = docs.split('\\t')\n",
    "\n",
    "    docs = ' '.join(ts)\n",
    "  \n",
    "    # Tokenization\n",
    "    tokens = WordPunctTokenizer().tokenize(docs)\n",
    "\n",
    "    # lowercase\n",
    "    tokens_lowercase = [ w.lower() for w in tokens]\n",
    "    \n",
    "    #Remove Stop words\n",
    "    tokens_stop  = [ w for w in tokens_lowercase if(w not in stop_ws)] \n",
    "    \n",
    "    # Stemming \n",
    "    tokens_stem = [ PorterStemmer().stem(w) for w in tokens_stop]   # .wes. we\n",
    "\n",
    "    # Lemmatization\n",
    "    updated_tokens = [ WordNetLemmatizer().lemmatize(w) for w in tokens_stem]\n",
    "     \n",
    "    final_tokens = []\n",
    "    \n",
    "    for updated_token in updated_tokens:\n",
    "        if(updated_token[0].isalpha()) and (len(updated_token) > 1):\n",
    "            final_tokens.append(updated_token)\n",
    "        else:\n",
    "            if(updated_token.isnumeric()):\n",
    "                final_tokens.append(num2words(Decimal(updated_token)))\n",
    "                \n",
    "#                 final_tokens.append(str(int(updated_token)))\n",
    "#             else:\n",
    "#                 if(updated_token[0].isdigit()):\n",
    "#                     updated_token = updated_token.lstrip('0')\n",
    "#                     final_tokens.append(updated_token)\n",
    "        \n",
    "    \n",
    "    final_tokens = final_tokens[1:]  # remove b\n",
    "\n",
    "    return final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeInput(inp):\n",
    "    \"\"\"\n",
    "        inp : Not a stop word\n",
    "        \n",
    "        0. Convert to lowercase\n",
    "        1. Stemming\n",
    "        2. Lemmatization\n",
    "        3. Only words that starts with alphabet or digit.\n",
    "    \"\"\"\n",
    "    \n",
    "    if(inp.isnumeric()):\n",
    "#         print('y')\n",
    "        return num2words(Decimal(inp))\n",
    "    \n",
    "    try:\n",
    "        if(isinstance(w2n.word_to_num(inp), int)):\n",
    "            return inp\n",
    "    \n",
    "    except ValueError:\n",
    "        pass\n",
    "       \n",
    "    \n",
    "    # lowercase\n",
    "    inp = inp.lower()\n",
    "    \n",
    "    # Stemming \n",
    "    inp_stem = PorterStemmer().stem(inp)\n",
    "    \n",
    "    # Lemmatization\n",
    "    inp_lemma = WordNetLemmatizer().lemmatize(inp_stem)\n",
    "    \n",
    "    inp_lemma = inp_lemma.lstrip('0')\n",
    "    \n",
    "    #strip spaces \n",
    "    inp_lemma = inp_lemma.strip()\n",
    "    \n",
    "    return inp_lemma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Inverted Index ####\n",
    "\n",
    "def makeIndex(token_docid):\n",
    "    inverted_index = {}\n",
    "\n",
    "    for element in token_docid:\n",
    "        term = element[0]\n",
    "        docid = element[1]\n",
    "        if(term not in inverted_index.keys()):\n",
    "            postings_list = [0,{}]  # df, key = docid, value = tf\n",
    "            postings_list[0] = 1\n",
    "            postings_list[1][docid] = 1 #tf \n",
    "            inverted_index[term] = postings_list\n",
    "        else:\n",
    "            plist = inverted_index[term] \n",
    "            if docid not in plist[1].keys():\n",
    "                plist[1][docid] = 1 #tf \n",
    "                plist[0]+=1\n",
    "            else:\n",
    "                plist[1][docid]+=1\n",
    "\n",
    "    return inverted_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryTermsOR(qts, title_index, text_index):\n",
    "    docs_found = []\n",
    "    \n",
    "    for qt in qts:\n",
    "        if(qt in text_index.keys()):\n",
    "            for doc_id in text_index[qt][1].keys():\n",
    "                if(doc_id not in docs_found):\n",
    "                    docs_found.append([qt, doc_id])\n",
    "                    \n",
    "        if(qt in title_index.keys()):\n",
    "            for doc_id in title_index[qt][1].keys():\n",
    "                if(doc_id not in docs_found):\n",
    "                    docs_found.append([qt, doc_id])\n",
    "            \n",
    "    return docs_found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResults(query_docs, title_index, text_index):\n",
    "    score = []\n",
    "\n",
    "    for qt_docid in query_docs:\n",
    "        a = 0\n",
    "        b = 0\n",
    "        #check qt_docid[1] is from title or text index\n",
    "        \n",
    "        if qt_docid[0] in title_index.keys():\n",
    "            \n",
    "            if qt_docid[1] in title_index[qt_docid[0]][1].keys():\n",
    "                \n",
    "                tf_title = 1 + math.log(title_index[qt_docid[0]][1][qt_docid[1]], 10)\n",
    "                idf_title = math.log(N/title_index[qt_docid[0]][0], 10)\n",
    "                a = 0.7 * (tf_title) * (idf_title)\n",
    "            \n",
    "        else:\n",
    "            a = 0\n",
    "        \n",
    "\n",
    "        if qt_docid[0] in text_index.keys():  \n",
    "            \n",
    "            if qt_docid[1] in text_index[qt_docid[0]][1].keys():\n",
    "                \n",
    "                tf_all = 1 + math.log(text_index[qt_docid[0]][1][qt_docid[1]], 10)\n",
    "                idf_all = math.log(N/text_index[qt_docid[0]][0], 10)\n",
    "\n",
    "                if(a!=0):\n",
    "                    tf_body = tf_all - tf_title\n",
    "                    idf_body = idf_all - idf_title\n",
    "                    b = 0.3 * (tf_body * idf_body)\n",
    "                else:\n",
    "                    tf_body = tf_all \n",
    "                    idf_body = idf_all \n",
    "                    b = 0.3 * (tf_body * idf_body)\n",
    "              \n",
    "        else:\n",
    "            b = 0\n",
    "\n",
    "        s = a + b\n",
    "        score.append([s, qt_docid[0],qt_docid[1]])   # tfidf, term, doc_id\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docsVec():\n",
    "    global tdm\n",
    "    k = 0\n",
    "    for term, plist in text_inverted_index.items():  # every term,plist \n",
    "        for i in range(1,N+1):      # every doc id\n",
    "            # calculate tf idf of term, for every doc id\n",
    "            if i in plist[1].keys():\n",
    "                c = getResults([[term, i]], title_inverted_index, text_inverted_index) # tfidf, term, doc_id\n",
    "                tdm[k][i-1] = c[0][0]\n",
    "        k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryVec(query_tokens):\n",
    "    \n",
    "    query_vec = np.zeros((V,1))\n",
    "    k= 0\n",
    "    unq, cnt = np.unique(query_tokens, return_counts = True)\n",
    "\n",
    "    for term in text_inverted_index.keys():\n",
    "        if(term in query_tokens): \n",
    "            # Query vector # find index of  term in unq then get cnt[index]\n",
    "            pos = np.nonzero(unq == term)[0][0]\n",
    "            tf_query = 1 + math.log(cnt[pos], 10) \n",
    "            \n",
    "            df_from_textIn = text_inverted_index[term][0]\n",
    "            \n",
    "            if(term in title_inverted_index.keys()):\n",
    "                df_from_titleIn = title_inverted_index[term][0]\n",
    "            else:\n",
    "                df_from_titleIn = 0\n",
    "            \n",
    "            body_df = df_from_textIn - df_from_titleIn\n",
    "#             print(df_from_textIn, df_from_titleIn)\n",
    "            if(df_from_titleIn!=0 and body_df!=0):\n",
    "                \n",
    "                idf_query = 0.7 * math.log(N/df_from_titleIn, 10) + 0.3 * (math.log(N/body_df, 10))\n",
    "            else:\n",
    "                idf_query = math.log(N/df_from_textIn, 10)\n",
    "                \n",
    "            query_vec[k] = tf_query * idf_query\n",
    "        else:\n",
    "            query_vec[k] = 0\n",
    "        k+=1\n",
    "    \n",
    "    return query_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  displayResult(ans, r, cs = False):\n",
    "    top_answer = sorted(ans, reverse=True) # Top 10 results\n",
    "    # print(top_answer)\n",
    "    best_ans = []\n",
    "\n",
    "    for ik in top_answer:\n",
    "        if(ik not in best_ans):\n",
    "            best_ans.append(ik)\n",
    "\n",
    "    if(cs):\n",
    "        show_result = pd.DataFrame(best_ans, columns =[\"score\",\"docid\"])\n",
    "    else:\n",
    "        show_result = pd.DataFrame(best_ans, columns =[\"score\", \"qterm\", \"docid\"])\n",
    "        \n",
    "    show_docName = show_result.iloc[:,-1].unique()[:r]\n",
    "    \n",
    "    print(\"\")\n",
    "    for docid in show_docName:\n",
    "        print(docs_table.iloc[docid-1, 1])\n",
    "    print(\"\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get strories title from index.html\n",
    "stories_path = os.getcwd() + \"/stories/\"\n",
    "index_path = os.getcwd() + \"/stories/index/\"\n",
    "\n",
    "docs_table  = getData(index_path)\n",
    "N = docs_table.shape[0]\n",
    "\n",
    "\n",
    "token_docId = []\n",
    "tk_docId = []\n",
    "\n",
    "\n",
    "for i in range(0, docs_table.shape[0]):\n",
    "    doc_id = docs_table.iloc[i, 0]\n",
    "    title = docs_table.iloc[i, 2]\n",
    "    content = docs_table.iloc[i, 3]\n",
    "    title_tokens = tokenizeDocument(title)\n",
    "    content_tokens = tokenizeDocument(content)\n",
    "\n",
    "    for token in title_tokens:\n",
    "        token_docId.append([token, doc_id])\n",
    "        \n",
    "    for token in content_tokens:\n",
    "        tk_docId.append([token, doc_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Title Inverted Index\n",
    "title_inverted_index = makeIndex(token_docId)\n",
    "\n",
    "text_inverted_index = makeIndex(tk_docId)\n",
    "\n",
    "# pprint(text_inverted_index['dragon'])\n",
    "\n",
    "# print(len(text_inverted_index), len(title_inverted_index))\n",
    "\n",
    "V = len(text_inverted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. Simple tf-idf\n",
    "def q1(query):\n",
    "    \n",
    "    query_terms = query.split(' ')\n",
    "    query_tokens = [ tokenizeInput(term) for term in query_terms ]\n",
    "\n",
    "#     print(query_tokens)\n",
    "    query_docs = queryTermsOR(query_tokens, title_inverted_index, text_inverted_index)\n",
    "    # print(query_docs)\n",
    "\n",
    "    ans = getResults(query_docs, title_inverted_index, text_inverted_index)\n",
    "\n",
    "    # Take input from user\n",
    "    k = 10\n",
    "\n",
    "    #show docs names\n",
    "    displayResult(ans, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. VSM\n",
    "\n",
    "# Term Document Matrix\n",
    "tdm = np.zeros((V,N))\n",
    "docsVec()\n",
    "# Normalize document vectors\n",
    "euclid_dist = np.linalg.norm(tdm, axis=0)\n",
    "# print(euclid_dist)\n",
    "# new_dist = (1/euclid_dist)\n",
    "\n",
    "for c in range(0, tdm.shape[1]):\n",
    "#     tdm[:,c] = new_dist[c]*tdm[:,c]    \n",
    "    tdm[:,c] = tdm[:,c]/euclid_dist[c]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2(query):\n",
    "\n",
    "    query_terms = query.split(' ')\n",
    "    query_tokens = [ tokenizeInput(term) for term in query_terms if(term not in stop_ws) ]\n",
    "#     print(query_tokens)\n",
    "    query_docs = queryTermsOR(query_tokens, title_inverted_index, text_inverted_index)\n",
    "\n",
    "    #Query Vecor\n",
    "    query_vec = queryVec(query_tokens)\n",
    "    \n",
    "\n",
    "    # normalize query vector\n",
    "    euclid_dist_qy = np.linalg.norm(query_vec)\n",
    "    # print(euclid_dist_qy)\n",
    "    query_vec = (1/euclid_dist_qy)*query_vec\n",
    "    # print(query_vec.T[0])\n",
    "    query_vec_new = np.array(list(query_vec.flat))\n",
    "    # Cosine similarity\n",
    "    possible_ans = [ qt_docid[1] for qt_docid in query_docs]\n",
    "\n",
    "    similarity = []\n",
    "    for doc_id in possible_ans:\n",
    "        match_score = np.inner(tdm[:,doc_id-1], query_vec_new)\n",
    "        similarity.append([match_score, doc_id])\n",
    "        \n",
    "    top_ans = sorted(similarity, reverse=True)\n",
    "    displayResult(top_ans, 10, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Tf-idf\n",
      "2. Cosine similarity with tf-idf\n",
      "Choose search type : 1\n",
      "Search : The Adveniure of the Three Gables\n",
      "\n",
      "3gables.txt\n",
      "lionmane.txt\n",
      "wisteria.txt\n",
      "rocket.sf\n",
      "darkness.txt\n",
      "ltp\n",
      "descent.poe\n",
      "hound-b.txt\n",
      "hitch3.txt\n",
      "cybersla.txt\n",
      "\n",
      "1. Tf-idf\n",
      "2. Cosine similarity with tf-idf\n",
      "Choose search type : 2\n",
      "Search : The Adveniure of the Three Gables\n",
      "\n",
      "3gables.txt\n",
      "lionmane.txt\n",
      "wisteria.txt\n",
      "hound-b.txt\n",
      "hitch3.txt\n",
      "cybersla.txt\n",
      "\n",
      "1. Tf-idf\n",
      "2. Cosine similarity with tf-idf\n",
      "Choose search type : 1\n",
      "Search : for the news which Lestrde would bring\n",
      "\n",
      "6napolen.txt\n",
      "breaks2.asc\n",
      "robotech\n",
      "bureau.txt\n",
      "hitch2.txt\n",
      "hellmach.txt\n",
      "radar_ra.txt\n",
      "sre04.txt\n",
      "rocket.sf\n",
      "outcast.dos\n",
      "\n",
      "1. Tf-idf\n",
      "2. Cosine similarity with tf-idf\n",
      "Choose search type : 2\n",
      "Search : for the news which Lestrde would bring\n",
      "\n",
      "6napolen.txt\n",
      "adv_alad.txt\n",
      "bullove.txt\n",
      "aminegg.txt\n",
      "crabhern.txt\n",
      "fleas.txt\n",
      "valen\n",
      "empnclot.txt\n",
      "bran\n",
      "weeprncs.txt\n",
      "\n",
      "1. Tf-idf\n",
      "2. Cosine similarity with tf-idf\n"
     ]
    }
   ],
   "source": [
    "run = True\n",
    "while(run):\n",
    "    print(\"1. Tf-idf\")\n",
    "    print(\"2. Cosine similarity with tf-idf\")\n",
    "\n",
    "    t = input('Choose search type : ')\n",
    "\n",
    "    if(t.isnumeric()):\n",
    "        if (int(t)== 1):\n",
    "            query = input(\"Search : \")\n",
    "            q1(query)\n",
    "            \n",
    "        elif(int(t) == 2):\n",
    "            query = input(\"Search : \")\n",
    "            q2(query)\n",
    "            \n",
    "        else:\n",
    "            run = False\n",
    "            print(\"Exiting...\")\n",
    "    else:\n",
    "        run = False\n",
    "        print(\"Exiting...\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
