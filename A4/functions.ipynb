{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rename_documents(dataPath):\n",
    "\n",
    "    \"\"\" dataPath : \"20_newsgroups/\" \"\"\"\n",
    "    new_path = os.getcwd()+'/'+dataPath\n",
    "    every_path = list(os.walk(new_path))\n",
    "    \n",
    "    total = 1\n",
    "    for i in range(0,len(every_path)):\n",
    "        dirPath, dirName, fileNames = every_path[i]\n",
    "        if(len(dirName) == 0):\n",
    "            for j in fileNames:\n",
    "                single_doc_loc = dirPath + '/' + j\n",
    "                os.rename(single_doc_loc, dirPath + '/' + str(total))\n",
    "                total+=1\n",
    "    \n",
    "    return     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testDataTokens(test_doc):\n",
    "    test_docs = []\n",
    "    \n",
    "    for doc in test_doc:\n",
    "        test_docs.append(tokenizeDocument(doc))\n",
    "        \n",
    "    return test_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_documents(dataPath):\n",
    "\n",
    "    docs = {}\n",
    "    labels = []\n",
    "    \n",
    "    new_path = os.getcwd()+'/'+dataPath\n",
    "    every_path = list(os.walk(new_path))\n",
    "    \n",
    "    f = 0\n",
    "    for i in range(0,len(every_path)):\n",
    "        dirPath, dirName, fileNames = every_path[i]\n",
    "        if(len(dirName) == 0):\n",
    "            for j in fileNames:\n",
    "                single_doc = []\n",
    "                single_doc_loc = dirPath + '/' + j\n",
    "                with open(single_doc_loc, 'rb') as f:\n",
    "                    single_doc.append(str(f.read()))\n",
    "                docs[j] = single_doc\n",
    "                labels.append(dirPath.split('/').pop())\n",
    "    return docs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Inverted Index ####\n",
    "\n",
    "def makeIndex(token_docid, C):\n",
    "    inverted_index = {}   # {token:[df,{'class i': 'token occurence count in class i'}]}\n",
    "    class_index = {}   # {class i: total no. of tokens }\n",
    "    \n",
    "    for c in range(0, C):\n",
    "        class_index[c] = 0\n",
    "        \n",
    "    for i in range(0, token_docid.shape[0]):\n",
    "        term = token_docid.loc[i, 'token']\n",
    "        doclabel = token_docid.loc[i, 'class']\n",
    "        \n",
    "        if(term not in inverted_index.keys()):\n",
    "            postings_list = [0,{}]  # df, key = docid, value = tf\n",
    "            postings_list[0] = 1\n",
    "            postings_list[1][doclabel] = 1 #tf \n",
    "            inverted_index[term] = postings_list\n",
    "            class_index[doclabel]+=1\n",
    "            \n",
    "        else:\n",
    "            plist = inverted_index[term] \n",
    "            if doclabel not in plist[1].keys():\n",
    "                plist[1][doclabel] = 1 #tf \n",
    "                plist[0]+=1\n",
    "                class_index[doclabel]+=1\n",
    "            else:\n",
    "                plist[1][doclabel]+=1\n",
    "                class_index[doclabel]+=1\n",
    "\n",
    "    return inverted_index, class_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Inverted Index ####\n",
    "\n",
    "def makeIndexQ2(token_docid):\n",
    "    inverted_index = {}   # {token:[df,cf, {'doc_id': 'token occurence count in document doc_id'}]}\n",
    "    \n",
    "    for i in range(0, token_docid.shape[0]):\n",
    "        term = token_docid.loc[i, 'token']\n",
    "        docid = token_docid.loc[i, 'doc_id']\n",
    "        \n",
    "        if(term not in inverted_index.keys()):\n",
    "            postings_list = [0,0,{}]  # df, key = docid, value = tf\n",
    "            postings_list[0] = 1\n",
    "            postings_list[1]+=1\n",
    "            postings_list[2][docid] = 1 #tf \n",
    "            inverted_index[term] = postings_list\n",
    "        else:\n",
    "            plist = inverted_index[term] \n",
    "            if docid not in plist[2].keys():\n",
    "                plist[2][docid] = 1 #tf \n",
    "                plist[0]+=1\n",
    "                plist[1]+=1\n",
    "            else:\n",
    "                plist[2][docid]+=1\n",
    "                plist[1]+=1\n",
    "\n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeDocument(docs, qt = False):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            docs (dict) : all documents\n",
    "        return:\n",
    "            final tokens (list) : terms for index\n",
    "\n",
    "        0. Convert to lowercase\n",
    "        1. Stop words removed\n",
    "        2. Tokenize \n",
    "        3. Stemming\n",
    "        4. Lemmatization\n",
    "        5. Only words that starts with alphabet or digit. Front 0's removed.\n",
    "    \"\"\"\n",
    "\n",
    "    stop_ws = set(stopwords.words('english'))\n",
    "\n",
    "    ts = docs.split('\\\\n')\n",
    "    docs = ' '.join(ts)\n",
    "    ts = docs.split('\\t')\n",
    "\n",
    "    docs = ' '.join(ts)\n",
    "  \n",
    "    # Tokenization\n",
    "    tokens = WordPunctTokenizer().tokenize(docs)\n",
    "    \n",
    "    ntokens = []\n",
    "    tokenizer = RegexpTokenizer(r'[^_]')\n",
    "    for t in tokens:\n",
    "        ntokens.append(''.join(tokenizer.tokenize(t)))\n",
    "\n",
    "    # lowercase\n",
    "    tokens_lowercase = [ w.lower() for w in ntokens]\n",
    "    \n",
    "    #Remove Stop words\n",
    "    tokens_stop  = [ w for w in tokens_lowercase if(w not in stop_ws)] \n",
    "    \n",
    "#     tokens_stem = [ PorterStemmer().stem(w) for w in tokens_stop]\n",
    "    \n",
    "    # Lemmatization\n",
    "    updated_tokens = [ WordNetLemmatizer().lemmatize(w) for w in tokens_stop]\n",
    "     \n",
    "    final_tokens = []\n",
    "    updated_tokens = list(filter(None, updated_tokens))\n",
    "    \n",
    "    for updated_token in updated_tokens:\n",
    "        if(updated_token.isalpha() and len(updated_token) > 1 and  d.check(updated_token)):\n",
    "            final_tokens.append(updated_token)\n",
    "#         else:\n",
    "#             if(updated_token.isnumeric()):\n",
    "#                 final_tokens.append(str(int(updated_token)))\n",
    "                \n",
    "    if(not qt):\n",
    "        final_tokens = final_tokens[1:]  # remove b\n",
    "    else:\n",
    "        return final_tokens  \n",
    "\n",
    "    return final_tokens\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataset(data, a):\n",
    "    \"\"\"\n",
    "    Randomly split dataset\n",
    "    \"\"\"\n",
    "    nr = int((a/100)*(data.shape[0]/C))\n",
    "    train_data = data.groupby('class').head(nr)\n",
    "    test_data = pd.concat([data, train_data]).drop_duplicates(keep=False)\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertLabels(train_data, test_data):\n",
    "    doc_type_ct = train_data['class'].value_counts()\n",
    "    print(doc_type_ct)\n",
    "    docs_type = sorted(doc_type_ct.index.tolist())\n",
    "    \n",
    "    print(docs_type)\n",
    "    \n",
    "    priors_probab = [ doc_type_ct[i]/doc_type_ct.sum() for i in docs_type]\n",
    "    print(priors_probab)\n",
    "    # convert labels to numbers\n",
    "    train_data['class'].replace(docs_type, [0,2,3,4,1], inplace = True)\n",
    "    test_data['class'].replace(docs_type, [0,2,3,4,1], inplace = True)\n",
    "    \n",
    "    return train_data, test_data, priors_probab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNB(inverted_index):\n",
    "    \"\"\"\n",
    "        Calculate p(xi | y) for each feature for each class label in the training data\n",
    "    \"\"\"\n",
    "    likelihood = np.zeros((C, len(inverted_index.keys())))\n",
    "    total_tokens = len(inverted_index.keys())\n",
    "    \n",
    "    t = 0\n",
    "    for token, plist in inverted_index.items():\n",
    "        ll = []\n",
    "        for c in range(0,C):\n",
    "            if(c in plist[1].keys()):\n",
    "                ll.append((plist[1][c] + 1)/(class_index[c] + total_tokens))\n",
    "            else:\n",
    "                ll.append(1/(class_index[c] + total_tokens))\n",
    "        \n",
    "        likelihood[:, t] = ll\n",
    "        t+=1\n",
    "\n",
    "    likelihoods = pd.DataFrame(likelihood, columns = list(inverted_index.keys()))\n",
    "    return likelihoods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAccuracy(predicted_class, test_data):\n",
    "    cm = np.zeros((C,C))\n",
    "    \n",
    "    for i in range(0, len(predicted_class)):\n",
    "        cm[predicted_class[i], test_data.loc[i, 'class']]+=1\n",
    "\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureSelect(index, total_docs):\n",
    "    term_util = []\n",
    "    \n",
    "    out = 0\n",
    "    for token, plist in index.items():\n",
    "        clear_output(wait = True)\n",
    "        df = plist[0]\n",
    "        for doc,tf in plist[2].items():\n",
    "            tf_idf = (1 + math.log(plist[1],10)) * math.log(total_docs/df,10)\n",
    "            term_util.append([token, tf_idf])\n",
    "        print('Current progress: ', np.round((out/len(inverted_index.keys()))*100, 2))\n",
    "        out+=1\n",
    "        \n",
    "    return term_util\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawConfusionMatrix(cm, title = False):\n",
    "    predict_ll = ['comp.graphics', 'rec.sport.hockey', 'sci.med', 'sci.space', 'talk.politics.misc']\n",
    "#     actual_ll = [ 'Actual '+str(i) for i in range(0, C)]\n",
    "    \n",
    "    hm = pd.DataFrame(cm, index = predict_ll, columns = predict_ll)\n",
    "    sns.heatmap(hm, annot = True, fmt='g', cmap = \"YlGnBu\")\n",
    "    plt.title('Confusion Matrix for '+title)\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.show()\n",
    "    return\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runQ2(feat, reduced_vocab_terms):\n",
    "   \n",
    "    new_inv = {}\n",
    "    \n",
    "    for token in reduced_vocab_terms:\n",
    "        new_inv[token] = inverted_index[token]\n",
    "    \n",
    "    new_likelihood = trainNB(new_inv)\n",
    "    new_predicts = classify(test_docs, new_inv, new_likelihood)\n",
    "    new_conf_mat = findAccuracy(new_predicts, test_data)\n",
    "    \n",
    "    acc = (np.trace(new_conf_mat)/np.sum(new_conf_mat))*100\n",
    "    print('Accuracy for ',str(split_percentage) +':'+ str(100 - split_percentage), ' with top '+str(feat)+'% features: ', str(acc)+'%.')\n",
    "#     drawConfusionMatrix(new_conf_mat, 'top ' + str(feat) +'% features')\n",
    "    return acc\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
